{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_Scrape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiGE/n6Cr2p+yWo5k9xjZ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephedward/Libridex/blob/master/Test_Scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MuoIyVVu-FY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "714d1237-7ef6-4e3c-8405-537107ada4f3"
      },
      "source": [
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install flask\n",
        "!pip install pandas\n",
        "!pip install bs4\n",
        "!pip install splinter\n",
        "!pip install nltk\n",
        "!pip install sklearn\n",
        "!pip install gensim\n",
        "!pip install matplotlib\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/84/23ed6a1796480a6f1a2d38f2802901d078266bda38388954d01d3f2e821d/pip-20.1.1-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.1.1\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask) (2.11.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask) (1.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n",
            "Collecting splinter\n",
            "  Downloading splinter-0.13.0.tar.gz (25 kB)\n",
            "Collecting selenium>=3.141.0\n",
            "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
            "\u001b[K     |████████████████████████████████| 904 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from splinter) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium>=3.141.0->splinter) (1.24.3)\n",
            "Building wheels for collected packages: splinter\n",
            "  Building wheel for splinter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for splinter: filename=splinter-0.13.0-py3-none-any.whl size=33302 sha256=3edc2742a060be603f7c2fc57343238d5805c895cd0acddcc0c16a01f0b7c2cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/73/71/90784f7e46ba2b041402409bb31b57eeeb84d6ee9c1035fdbb\n",
            "Successfully built splinter\n",
            "Installing collected packages: selenium, splinter\n",
            "Successfully installed selenium-3.141.0 splinter-0.13.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.24 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.24)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.24->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.24->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDm44_Htu0cV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "b56200c5-f9b8-44b8-bd77-3b5003cdc546"
      },
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time \n",
        "from splinter import Browser\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Word2Vec \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings;\n",
        "import requests\n",
        "# import untangle\n",
        "# warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBwZZ023uhsH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "b04ef51d-a5be-4bd3-cbcb-1f0bf05d8310"
      },
      "source": [
        "                   \n",
        "            # Function for removing NonAscii characters\n",
        "def _removeNonAscii(s):\n",
        "              return \"\".join(i for i in s if  int(i)<128)# Function for converting into lower case\n",
        "def make_lower_case(text):\n",
        "              return str(text).lower()# Function for removing stop words\n",
        "def remove_stop_words(text):\n",
        "              text = text.split()\n",
        "              stops = set(stopwords.words(\"english\"))\n",
        "              text = [w for w in text if not w in stops]\n",
        "              text = \" \".join(text)\n",
        "              return text# Function for removing punctuation\n",
        "def remove_punctuation(text):\n",
        "              tokenizer = RegexpTokenizer(r'\\w+')\n",
        "              text = tokenizer.tokenize(text)\n",
        "              text = \" \".join(text)\n",
        "              return text\n",
        "#Function for removing the html tags\n",
        "def remove_html(text):\n",
        "              html_pattern = re.compile('<.*?>')\n",
        "              return html_pattern.sub(r'', text)# Applying all the functions in description and storing as a cleaned_desc\n",
        "\n",
        "print(\"cleaning book list\")\n",
        "book_df = pd.read_csv('https://raw.githubusercontent.com/josephedward/Libridex_Recommendation_Engine/master/resources/book_obj_list_v2(1).csv')\n",
        "book_df['cleaned_desc'] = book_df['description'].apply(func = make_lower_case)\n",
        "# testdf['cleaned_desc'] = df.cleaned_desc.apply(func = make_lower_case)\n",
        "book_df['cleaned_desc'] = book_df.cleaned_desc.apply(func = remove_stop_words)\n",
        "book_df['cleaned_desc'] = book_df.cleaned_desc.apply(func=remove_punctuation)\n",
        "book_df['cleaned_desc'] = book_df.cleaned_desc.apply(func=remove_html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rec_whole_objs=[]\n",
        "\n",
        "            \n",
        "            \n",
        "            \n",
        "for x in range(16000):\n",
        "    print(\"Searching Librivox ID: \",x)\n",
        "    librivox_id_search = f'https://librivox.org/api/feed/audiobooks/?id={x}'            \n",
        "# Use requests to retrieve data from a given URL\n",
        "# Parse the whole HTML page using BeautifulSoup\n",
        "    # librivox_id_search = 'https://librivox.org/api/feed/audiobooks/?id=52'\n",
        "    try:\n",
        "      lbvx_response = requests.get(librivox_id_search)\n",
        "      soup = BeautifulSoup(lbvx_response.text, 'html.parser')\n",
        "      lib_id=soup.find(\"id\").get_text()\n",
        "      title=soup.find(\"title\").get_text()\n",
        "      description=soup.find(\"description\").get_text()\n",
        "      language=soup.find(\"language\").get_text()\n",
        "      copyright_year=soup.find(\"copyright_year\").get_text()\n",
        "      lib_book_url=soup.find('url_librivox').get_text()\n",
        "      author_fname= soup.find('author').find('first_name').get_text()\n",
        "      author_lname= soup.find('author').find('last_name').get_text()\n",
        "      author= author_fname+\" \"+author_lname\n",
        "      book_page_resp = requests.get(lib_book_url)\n",
        "      soup = BeautifulSoup(book_page_resp.text, 'html.parser')\n",
        "      genre=soup.find_all('p', class_='book-page-genre')[0].get_text()\n",
        "      genre_arr=genre.split(\":\")\n",
        "      genre=genre_arr[1]\n",
        "      # book_objs.append\n",
        "      book_scrape_df = pd.DataFrame([{\"lib_id\":lib_id,\n",
        "                  \"title\":title,\n",
        "                  \"author\":author,\n",
        "                  \"genre\":genre,\n",
        "                  \"description\":description,\n",
        "                  \"language\":language,\n",
        "                  \"copyright_year\":copyright_year,\n",
        "                  \"lib_book_url\":lib_book_url                  \n",
        "                      }])\n",
        "      print(book_scrape_df)\n",
        "      runFlag = True\n",
        "    except:\n",
        "      print(\"couldn'y locate : \",x )\n",
        "      runFlag = False\n",
        "    # Matching the language with the dataset and reset the index\n",
        "    \n",
        "    print(\"running analyis on: \", title)\n",
        "    if runFlag==True:\n",
        "      try:\n",
        "        data = book_df.loc[book_df['language'] == \"English\"]  \n",
        "        # data = \n",
        "        data.reset_index(level = 0, inplace = True) \n",
        "        print('Convert the index into series')\n",
        "        indices = pd.Series(data.index, index = data['title'])\n",
        "        # print(indices)\n",
        "        print(\"Converting the book title into vectors and used bigram\")\n",
        "        tf = TfidfVectorizer(analyzer='word', ngram_range=(2, 2), min_df = 1, stop_words='english')\n",
        "        # print(tf)\n",
        "        tfidf_matrix = tf.fit_transform(data['cleaned_desc'])\n",
        "        # print(tfidf_matrix)\n",
        "        print(\"Calculating the similarity measures based on Cosine Similarity\")\n",
        "        sg = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "        # print(sg)    \n",
        "        print(\"Get the index corresponding to original_title\")       \n",
        "        idx = indices[title]\n",
        "        # print(idx)\n",
        "        print('Get the pairwise similarity scores') \n",
        "        sig = list(enumerate(sg[idx]))\n",
        "        # print(sig)\n",
        "        print(\"Sort the books\")\n",
        "        sig = sorted(sig, key=lambda x: x[1].any(), reverse=True)\n",
        "        print('Scores of the 5 most similar books') \n",
        "        sig = sig[1:6]\n",
        "        # print(sig)\n",
        "        print(\"Book indicies\")\n",
        "        bk_indices = [i[0] for i in sig]\n",
        "        print(\"Top 5 book recommendations\")\n",
        "        # rec = data['lib_url'].iloc[bk_indices]   \n",
        "        rec=data.iloc[bk_indices]\n",
        "        print(rec.columns)\n",
        "        break \n",
        "      except:\n",
        "        print(\"couldn't analyze: \", title)\n",
        "\n",
        "# static_recs_path=\"../resources/combined_recommendations_v1.csv\"\n",
        "# recs_df=pd.read_csv(static_recs_path)\n",
        "\n",
        "# rec_objs_list=[]\n",
        "\n",
        "# for x in recs_df['book_title']:\n",
        "#     rec_arr = recs_df.loc[recs_df['book_title'] == x]['book_recommendation_urls'].values\n",
        "#     title = x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(title)\n",
        "    rec_arr = rec\n",
        "    print(\"Rec URL Arr : \", rec_arr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     pattern=r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))';\n",
        "#     print(\"matching URLS...\")\n",
        "#     match = re.findall(pattern, str(rec_arr))\n",
        "#     # match[1][0]\n",
        "#     rec_urls=[]\n",
        "#     for x in match:\n",
        "#         rec_urls.append(x[0])\n",
        "\n",
        "#     # Scrape\n",
        "    for x in rec_arr:\n",
        "            print(\"scraping :\", x)\n",
        "            x_req = requests.get(x)\n",
        "            search_page = x_req.text\n",
        "            soup = BeautifulSoup(search_page, 'html.parser')\n",
        "            book_page= soup.find('div', class_=\"page book-page\")\n",
        "            # title=book_page.find('h1').get_text()\n",
        "            # author=book_page.find('p', class_=\"book-page-author\").get_text()\n",
        "            img_url=book_page.find('div', class_=\"book-page-book-cover\").find('img').get('src')\n",
        "            rec_objs.append({\n",
        "                'title':title,\n",
        "                'author':author,\n",
        "                'img_url':img_url,\n",
        "                'page_url':x\n",
        "            })\n",
        "    print(\"done scrapion : \", title)\n",
        "    print(rec_objs)\n",
        "        \n",
        "#     rec_objs_list.append({\n",
        "#                 \"title\":title,\n",
        "#                 \"rec_objects\":rec_objs\n",
        "#             })\n",
        "#     print(\"appended\")\n",
        "#     rec_objs_df=pd.DataFrame(rec_objs_list)\n",
        "#     rec_objs_df.to_csv('rec_obj_list_v1.csv')\n",
        "#     print(\"written to csv\")\n",
        "\n",
        "\n",
        "# print(\"finished\")\n",
        "# print(rec_objs_list)\n",
        "\n",
        "\n",
        "# from google.colab import files\n",
        "# df.to_csv('filename.csv') \n",
        "# files.download('filename.csv')\n",
        "\n",
        "# # rec = data[['title']].iloc[movie_indices]\n",
        "# # rec_whole_df = book_df[book_df['title'].isin(rec['title'])]\n",
        "# # print(rec_whole_df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cleaning book list\n",
            "Searching Librivox ID:  0\n",
            "  lib_id  ...                                       lib_book_url\n",
            "0     47  ...  https://librivox.org/the-count-of-monte-cristo...\n",
            "\n",
            "[1 rows x 8 columns]\n",
            "running analyis on:  Count of Monte Cristo\n",
            "Convert the index into series\n",
            "Converting the book title into vectors and used bigram\n",
            "Calculating the similarity measures based on Cosine Similarity\n",
            "Get the index corresponding to original_title\n",
            "Get the pairwise similarity scores\n",
            "Sort the books\n",
            "Scores of the 5 most similar books\n",
            "Book indicies\n",
            "Top 5 book recommendations\n",
            "Index(['index', 'Unnamed: 0', 'lib_id', 'title', 'genre', 'description',\n",
            "       'language', 'copyright_year', 'lib_url', 'cleaned_desc'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4YGDCvUJz8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}